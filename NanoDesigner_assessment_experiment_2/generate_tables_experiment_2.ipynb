{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "main_dir = \"./NanoDesigner/NanoDesigner_assessment_experiment_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"Calculate confidence interval for the mean\"\"\"\n",
    "    if len(data) == 0 or np.all(np.isnan(data)):\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    clean_data = data[~np.isnan(data)]\n",
    "    if len(clean_data) == 0:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    n = len(clean_data)\n",
    "    mean = np.mean(clean_data)\n",
    "    std_err = stats.sem(clean_data)\n",
    "    margin_error = std_err * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    \n",
    "    return mean, margin_error\n",
    "\n",
    "def success_rate_confidence_interval(successes, total, confidence=0.95):\n",
    "    \"\"\"Calculate confidence interval for success rate (binomial proportion)\"\"\"\n",
    "    if total == 0:\n",
    "        return 0, np.nan\n",
    "    \n",
    "    p = successes / total\n",
    "    z = stats.norm.ppf((1 + confidence) / 2.)\n",
    "    margin = z * np.sqrt(p * (1 - p) / total)\n",
    "    \n",
    "    return p * 100, margin * 100  # Convert to percentage\n",
    "\n",
    "def load_json_lines(file_path):\n",
    "    \"\"\"Load JSON lines file\"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    data.append(json.loads(line.strip()))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "    return data\n",
    "\n",
    "def discover_pdb_directories(main_dir, experiment_subdir):\n",
    "    \"\"\"Discover all PDB directories (4 character alphanumeric IDs) in the experiment directory\"\"\"\n",
    "    experiment_path = Path(main_dir) / experiment_subdir\n",
    "    pdb_dirs = []\n",
    "    \n",
    "    if experiment_path.exists():\n",
    "        for item in experiment_path.iterdir():\n",
    "            if item.is_dir():\n",
    "                # Check if directory name matches 4 character alphanumeric pattern\n",
    "                if re.match(r'^[a-zA-Z0-9]{4}$', item.name):\n",
    "                    pdb_dirs.append(item.name)\n",
    "    \n",
    "    return sorted(pdb_dirs)\n",
    "\n",
    "def process_pooled_iteration_data(all_experiments_data, iteration_num):\n",
    "    \"\"\"\n",
    "    Process data for a specific iteration by pooling all designs across all PDPs\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for exp_key, exp_data in all_experiments_data.items():\n",
    "        if iteration_num not in exp_data:\n",
    "            print(f\"Warning: Iteration {iteration_num} not found for {exp_key}\")\n",
    "            continue\n",
    "            \n",
    "        iteration_data = exp_data[iteration_num]\n",
    "        \n",
    "        # Pool all candidates from all PDPs\n",
    "        pooled_metrics = {\n",
    "            'AAR H3': [],\n",
    "            'RMSD(CA) aligned': [],\n",
    "            'RMSD(CA) CDRH3 aligned': [],\n",
    "            'TMscore': [],\n",
    "            'LDDT': [],\n",
    "            'FoldX_dG': [],\n",
    "            'FoldX_ddG': [],\n",
    "            'final_num_clashes': [],\n",
    "            'DockQ': []\n",
    "        }\n",
    "        \n",
    "        total_valid_ddg = 0\n",
    "        total_negative_ddg = 0\n",
    "        \n",
    "        # Pool all individual candidate data\n",
    "        for pdb_candidates in iteration_data['all_candidates']:\n",
    "            for candidate in pdb_candidates:\n",
    "                # Extract metrics from individual candidates\n",
    "                for metric_key, metric_values in pooled_metrics.items():\n",
    "                    if metric_key in candidate and not pd.isna(candidate[metric_key]):\n",
    "                        metric_values.append(candidate[metric_key])\n",
    "                \n",
    "                # Count for success rate\n",
    "                if 'FoldX_ddG' in candidate and not pd.isna(candidate['FoldX_ddG']):\n",
    "                    total_valid_ddg += 1\n",
    "                    if candidate['FoldX_ddG'] < 0:\n",
    "                        total_negative_ddg += 1\n",
    "        \n",
    "        # Calculate statistics for pooled data\n",
    "        exp_results = {}\n",
    "        \n",
    "        for metric, values in pooled_metrics.items():\n",
    "            if values:\n",
    "                mean, ci = mean_confidence_interval(np.array(values))\n",
    "                exp_results[f'{metric}_mean'] = mean\n",
    "                exp_results[f'{metric}_ci'] = ci\n",
    "                exp_results[f'{metric}_n'] = len(values)\n",
    "            else:\n",
    "                exp_results[f'{metric}_mean'] = np.nan\n",
    "                exp_results[f'{metric}_ci'] = np.nan\n",
    "                exp_results[f'{metric}_n'] = 0\n",
    "        \n",
    "        # Success rate\n",
    "        if total_valid_ddg > 0:\n",
    "            success_rate, success_ci = success_rate_confidence_interval(total_negative_ddg, total_valid_ddg)\n",
    "            exp_results['Success_Rate_mean'] = success_rate\n",
    "            exp_results['Success_Rate_ci'] = success_ci\n",
    "            exp_results['Success_Rate_n'] = total_valid_ddg\n",
    "        else:\n",
    "            exp_results['Success_Rate_mean'] = np.nan\n",
    "            exp_results['Success_Rate_ci'] = np.nan\n",
    "            exp_results['Success_Rate_n'] = 0\n",
    "        \n",
    "        results[exp_key] = exp_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "def load_all_experiment_data(main_dir, experiment_configs):\n",
    "    \"\"\"Load data from all experiments and organize by iteration\"\"\"\n",
    "    \n",
    "    all_experiments_data = {}\n",
    "    \n",
    "    for exp_name, design_type, subdir in experiment_configs:\n",
    "        print(f\"Loading {exp_name} - {design_type}...\")\n",
    "        \n",
    "        # Discover PDB directories automatically\n",
    "        pdbs = discover_pdb_directories(main_dir, subdir)\n",
    "        print(f\"Found PDBs: {pdbs}\")\n",
    "        \n",
    "        exp_key = f\"{exp_name}_{design_type}\"\n",
    "        exp_data = defaultdict(lambda: {'all_candidates': []})\n",
    "        \n",
    "        for pdb in pdbs:\n",
    "            pdb_folder = Path(main_dir) / subdir / pdb\n",
    "            if not pdb_folder.exists():\n",
    "                continue\n",
    "            \n",
    "            # Process iterations 1-10\n",
    "            for iteration in range(1, 11):\n",
    "                json_file = pdb_folder / f\"best_candidates_iter_{iteration}.json\"\n",
    "                \n",
    "                if json_file.exists():\n",
    "                    iteration_data = load_json_lines(json_file)\n",
    "                    \n",
    "                    if iteration_data:\n",
    "                        # Take top 15 entries (or all if less than 15)\n",
    "                        top_entries = iteration_data[:15]\n",
    "                        \n",
    "                        # Store all candidates for pooled analysis\n",
    "                        exp_data[iteration]['all_candidates'].append(top_entries)\n",
    "        \n",
    "        all_experiments_data[exp_key] = exp_data\n",
    "    \n",
    "    return all_experiments_data\n",
    "\n",
    "def format_metric_value(mean, ci, metric_key, is_best=False, is_second=False):\n",
    "    \"\"\"Format metric value with confidence interval and highlighting\"\"\"\n",
    "    if pd.isna(mean) or pd.isna(ci):\n",
    "        return \"N/A\"\n",
    "    \n",
    "    # Format based on metric type\n",
    "    if metric_key == 'Success_Rate':\n",
    "        formatted = f\"{mean:.1f} ± {ci:.1f}\"\n",
    "    elif 'RMSD' in metric_key or 'FoldX' in metric_key:\n",
    "        formatted = f\"{mean:.2f} ± {ci:.2f}\"\n",
    "    else:\n",
    "        formatted = f\"{mean:.3f} ± {ci:.3f}\"\n",
    "    \n",
    "    # Add highlighting\n",
    "    if is_best:\n",
    "        return f\"\\\\textbf{{{formatted}}}\"\n",
    "    elif is_second:\n",
    "        return f\"\\\\underline{{{formatted}}}\"\n",
    "    else:\n",
    "        return formatted\n",
    "\n",
    "def determine_best_values_for_scenario(scenario_results, metric_key, higher_better):\n",
    "    \"\"\"Determine best and second best values for a specific scenario\"\"\"\n",
    "    values = {}\n",
    "    \n",
    "    # Extract mean values for comparison\n",
    "    for method in ['DiffAb', 'ADesigner']:\n",
    "        if method in scenario_results:\n",
    "            mean_val = scenario_results[method].get(f'{metric_key}_mean', np.nan)\n",
    "            if not pd.isna(mean_val):\n",
    "                values[method] = mean_val\n",
    "    \n",
    "    if len(values) < 2:\n",
    "        return {}, {}\n",
    "    \n",
    "    # Sort based on whether higher is better\n",
    "    sorted_methods = sorted(values.items(), key=lambda x: x[1], reverse=higher_better)\n",
    "    \n",
    "    best_tools = {sorted_methods[0][0]: True}\n",
    "    second_tools = {sorted_methods[1][0]: True}\n",
    "    \n",
    "    return best_tools, second_tools\n",
    "\n",
    "def generate_latex_table_for_scenario(baseline_results, ours_results, scenario_name, scenario_type):\n",
    "    \"\"\"Generate LaTeX table for a specific scenario comparing baseline vs ours\"\"\"\n",
    "    \n",
    "    # Define metrics with their display names and whether higher is better\n",
    "    metrics_info = {\n",
    "        'AAR H3': ('AAR H3', True),\n",
    "        'RMSD(CA) aligned': ('RMSD', False),\n",
    "        'RMSD(CA) CDRH3 aligned': ('RMSD CDRH3', False),\n",
    "        'TMscore': ('TMscore', True),\n",
    "        'LDDT': ('LDDT', True),\n",
    "        'FoldX_dG': ('$\\\\Delta G$', False),\n",
    "        'FoldX_ddG': ('$\\\\Delta \\\\Delta G$', False),\n",
    "        'final_num_clashes': ('Clashes', False),\n",
    "        'DockQ': ('DockQ', True),\n",
    "        'Success_Rate': ('Success Rate \\\\%', True)\n",
    "    }\n",
    "    \n",
    "    # Create caption and label\n",
    "    caption_text = f\"Performance metrics for NanoDesigner with {scenario_name} {scenario_type} scenario\"\n",
    "    label_text = f\"nanodesigner_{scenario_name.lower()}_{scenario_type.lower()}_metrics\"\n",
    "    \n",
    "    latex_lines = []\n",
    "    \n",
    "    # Table header\n",
    "    latex_lines.extend([\n",
    "        \"\\\\begin{table}[h]\",\n",
    "        \"\\\\centering\",\n",
    "        f\"\\\\caption{{{caption_text}}}\",\n",
    "        f\"\\\\label{{tab:{label_text}}}\",\n",
    "        \"\\\\begin{tabular}{l c c}\",\n",
    "        \"\\\\toprule\",\n",
    "        \"\\\\textbf{Metric} & \\\\textbf{Baseline} & \\\\textbf{Ours} \\\\\\\\\",\n",
    "        \"\\\\midrule\"\n",
    "    ])\n",
    "    \n",
    "    # Generate rows for each metric\n",
    "    for metric_key, (display_name, higher_better) in metrics_info.items():\n",
    "        # Get baseline and ours values\n",
    "        baseline_mean = baseline_results.get(f'{metric_key}_mean', np.nan)\n",
    "        baseline_ci = baseline_results.get(f'{metric_key}_ci', np.nan)\n",
    "        ours_mean = ours_results.get(f'{metric_key}_mean', np.nan)\n",
    "        ours_ci = ours_results.get(f'{metric_key}_ci', np.nan)\n",
    "        \n",
    "        # Determine which is better\n",
    "        is_baseline_better = False\n",
    "        is_ours_better = False\n",
    "        \n",
    "        if not pd.isna(baseline_mean) and not pd.isna(ours_mean):\n",
    "            if higher_better:\n",
    "                is_ours_better = ours_mean > baseline_mean\n",
    "                is_baseline_better = baseline_mean > ours_mean\n",
    "            else:\n",
    "                is_ours_better = ours_mean < baseline_mean\n",
    "                is_baseline_better = baseline_mean < ours_mean\n",
    "        \n",
    "        # Format values\n",
    "        baseline_formatted = format_metric_value(\n",
    "            baseline_mean, baseline_ci, metric_key, is_best=is_baseline_better\n",
    "        )\n",
    "        ours_formatted = format_metric_value(\n",
    "            ours_mean, ours_ci, metric_key, is_best=is_ours_better\n",
    "        )\n",
    "        \n",
    "        # Add direction indicator\n",
    "        direction = \"$\\\\uparrow$\" if higher_better else \"$\\\\downarrow$\"\n",
    "        display_name_with_direction = f\"{display_name} {direction}\"\n",
    "        \n",
    "        latex_lines.append(f\"{display_name_with_direction} & {baseline_formatted} & {ours_formatted} \\\\\\\\\")\n",
    "    \n",
    "    # Table footer\n",
    "    latex_lines.extend([\n",
    "        \"\\\\bottomrule\",\n",
    "        \"\\\\end{tabular}\",\n",
    "        \"\\\\end{table}\",\n",
    "        \"\"\n",
    "    ])\n",
    "    \n",
    "    return \"\\n\".join(latex_lines)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    # main_dir = \".rioszemm/NanoDesigner\"\n",
    "    \n",
    "    experiment_configs = [\n",
    "        ('ADesigner', 'Denovo', 'NanoDesigner_assessment_experiment_2/NanoDesigner_Adesigner_denovo'),\n",
    "        ('ADesigner', 'Optimization', 'NanoDesigner_assessment_experiment_2/NanoDesigner_Adesigner_optimization'),\n",
    "        ('DiffAb', 'Denovo', 'NanoDesigner_assessment_experiment_2/NanoDesigner_DiffAb_denovo'),\n",
    "        ('DiffAb', 'Optimization', 'NanoDesigner_assessment_experiment_2/NanoDesigner_DiffAb_optimization'),\n",
    "    ]\n",
    "    \n",
    "    # Load all data\n",
    "    all_experiments_data = load_all_experiment_data(main_dir, experiment_configs)\n",
    "    \n",
    "    # Get first iteration results (baseline)\n",
    "    baseline_results = process_pooled_iteration_data(all_experiments_data, 1)\n",
    "    \n",
    "    # Get last iteration results (ours)\n",
    "    last_iter_results = {}\n",
    "    last_iter_numbers = {}\n",
    "    \n",
    "    print(\"\\nFinding last iterations for each experiment:\")\n",
    "    for exp_key, exp_data in all_experiments_data.items():\n",
    "        available_iterations = [k for k in exp_data.keys() if isinstance(k, int)]\n",
    "        if available_iterations:\n",
    "            last_iteration = min(max(available_iterations), 10)  # Prefer 10th, but use highest available\n",
    "            last_iter_numbers[exp_key] = last_iteration\n",
    "            print(f\"{exp_key}: Last iteration = {last_iteration}\")\n",
    "            \n",
    "            last_results = process_pooled_iteration_data({exp_key: exp_data}, last_iteration)\n",
    "            last_iter_results.update(last_results)\n",
    "        else:\n",
    "            last_iter_numbers[exp_key] = 0\n",
    "            print(f\"{exp_key}: No data found\")\n",
    "    \n",
    "    # Generate LaTeX tables for each scenario\n",
    "    scenarios = [\n",
    "        ('DiffAb_Optimization', 'DiffAb', 'Optimization'),\n",
    "        ('DiffAb_Denovo', 'DiffAb', 'Denovo'),\n",
    "        ('ADesigner_Optimization', 'ADesigner', 'Optimization'),\n",
    "        ('ADesigner_Denovo', 'ADesigner', 'Denovo')\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"LATEX TABLES\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for exp_key, scenario_name, scenario_type in scenarios:\n",
    "        if exp_key in baseline_results and exp_key in last_iter_results:\n",
    "            table_latex = generate_latex_table_for_scenario(\n",
    "                baseline_results[exp_key],\n",
    "                last_iter_results[exp_key],\n",
    "                scenario_name,\n",
    "                scenario_type\n",
    "            )\n",
    "            print(f\"\\n% Table for {scenario_name} {scenario_type}\")\n",
    "            print(table_latex)\n",
    "        else:\n",
    "            print(f\"\\n% No data available for {scenario_name} {scenario_type}\")\n",
    "    \n",
    "    # Save detailed results to CSV\n",
    "    rows = []\n",
    "    for exp_key, data in baseline_results.items():\n",
    "        row = data.copy()\n",
    "        row['experiment_key'] = exp_key\n",
    "        row['iteration_type'] = 'baseline'\n",
    "        rows.append(row)\n",
    "    \n",
    "    for exp_key, data in last_iter_results.items():\n",
    "        row = data.copy()\n",
    "        row['experiment_key'] = exp_key\n",
    "        row['iteration_type'] = 'ours'\n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv('nanodesigner_baseline_vs_ours_comparison.csv', index=False)\n",
    "    print(f\"\\nDetailed results saved to: nanodesigner_baseline_vs_ours_comparison.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dymean0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
